{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "001.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "79D3o6bTqaZ8",
        "nmdsdh9xb6x1",
        "nlsHJu9GsWNE",
        "1K2RxdRItt0v"
      ],
      "authorship_tag": "ABX9TyORdxSotFI3UnuGiRL8zaCx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nithinivi/Deep_Learning_Discussion/blob/main/001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goV1jcDIp3vk"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "\n",
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmSqxL8EiPOm"
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data\n",
        "<br>\n",
        "![neural network](http://neuralnetworksanddeeplearning.com/images/tikz12.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79D3o6bTqaZ8"
      },
      "source": [
        "### Inside Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCYpXpicqRam"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2KaI9znp_sl"
      },
      "source": [
        "- SGD\n",
        "\n",
        "Gradient descent is a way to minimize an objective function $J(\\theta)$ parameterized by a model's parameters $\\theta \\in \\mathbb{R}^d$ by updating the parameters in the opposite direction of the gradient of the objective function $\\nabla_\\theta J(\\theta)$ w.r.t. to the parameters. The learning rate $\\eta$ determines the size of the steps we take to reach a  minimum. In other words, we follow the direction of the slope of the surface created by the objective function downhill until we reach a valley.\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta)\n",
        "\\end{equation}\n",
        "\n",
        "- Moementum\n",
        "\n",
        "Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations . It does this by adding a fraction $\\gamma$ of the update vector of the past time step to the current update vector\n",
        "\\begin{align}\n",
        "\\begin{split}\n",
        "v_t &= \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta)\\\\\n",
        "\\theta &= \\theta - v_t\n",
        "\\end{split}\n",
        "\\end{align}\n",
        "\n",
        "The momentum term $\\gamma$ is usually set to $0.9$ or a similar value.\n",
        "\n",
        "Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity, if there is air resistance, i.e. $\\gamma < 1$). The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.\n",
        "\n",
        "\n",
        "- Adam \n",
        "\n",
        "Adaptive Moment Estimation (Adam) is a method  that calculates  adaptive learning rates for each parameter. Adam stores an exponentially decaying average of past squared gradients $v_t$ l exponentially decaying average of past gradients $m_t$, similar to momentum:\n",
        "\n",
        " we set $g_{t, i}$ to be the gradient of the objective function w.r.t. to the parameter $\\theta_i$ at time step $t$:\n",
        "\n",
        "\\begin{equation}\n",
        "g_{t, i} = \\nabla_{\\theta_t} J( \\theta_{t,i} )\n",
        "\\end{equation}\n",
        "\n",
        "The SGD update for every parameter $\\theta_i$ at each time step $t$ then becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_{t+1, i} = \\theta_{t, i} - \\eta \\cdot g_{t, i}\n",
        "\\end{equation}\n",
        "   \n",
        "Moving variance $v_t$ and mean $m_t$\n",
        "\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\begin{split}\n",
        "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\\\\\n",
        "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
        "\\end{split}\n",
        "\\end{align}\n",
        "\n",
        "$m_t$ and $v_t$ are estimates of the  mean and variance of the gradients respectively. As $m_t$ and $v_t$ are initialized as vectors of $0$'s, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small \n",
        "\n",
        "They counteract these biases by computing bias-corrected first and second moment estimates:\n",
        "\n",
        "\\begin{align}\n",
        "\\begin{split}\n",
        "\\hat{m}_t &= \\frac{m_t}{1 - \\beta^t_1}\\\\\n",
        "\\hat{v}_t &= \\frac{v_t}{1 - \\beta^t_2}\n",
        "\\end{split}\n",
        "\\end{align}\n",
        "\n",
        "They then use these to update the parameters which yields the Adam update rule:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
        "\\end{equation}\n",
        "\n",
        "The authors propose default values of 0.9 for β1, 0.999 for β2, and 10−8\n",
        "for $\\epsilon$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img alt=\"\" class=\"pn ut et fh fd mn v c\" width=\"600\" height=\"458\" role=\"presentation\" src=\"https://miro.medium.com/max/600/1*U224pqhF4WUOZhfmDIWtxA.gif\" srcset=\"https://miro.medium.com/max/276/1*U224pqhF4WUOZhfmDIWtxA.gif 276w, https://miro.medium.com/max/552/1*U224pqhF4WUOZhfmDIWtxA.gif 552w, https://miro.medium.com/max/600/1*U224pqhF4WUOZhfmDIWtxA.gif 600w\" sizes=\"600px\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHR_4zS6sM7u"
      },
      "source": [
        "- What is softmax "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmdsdh9xb6x1"
      },
      "source": [
        "# Convolution Neural Networks\n",
        "\n",
        "- kamming paper (Imagenet )\n",
        "\n",
        "- what convolutions layer are learning (Fergerson Paper)\n",
        "\n",
        "- Max Pooling \n",
        "\n",
        "- batch Normalization \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-QxQDvPrBUI"
      },
      "source": [
        "\n",
        "![Convolution](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif \"Conv With Stride and padding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlsHJu9GsWNE"
      },
      "source": [
        "## Resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K2RxdRItt0v"
      },
      "source": [
        "# Refernces\n",
        "- http://neuralnetworksanddeeplearning.com/chap1.html\n",
        "- https://arxiv.org/pdf/1609.04747.pdf\n",
        "- http://cs231n.github.io/optimization-1/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8v5OA9Xtwc6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}